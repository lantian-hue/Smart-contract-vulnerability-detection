import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix
from sklearn.model_selection import train_test_split, GridSearchCV


filepath = "monitor2.csv"
data = pd.read_csv(filepath)

code=data["Opcode"].values
y=data["flag"].values
train, test, y_train, y_test = train_test_split(code, y, test_size=0.2,random_state=20)


#CountVectorizer()函数只考虑每个单词出现的频率；然后构成一个特征矩阵，每一行表示一个训练文本的词频统计结果。
vectorizer = CountVectorizer(ngram_range=(1, 2))
vectorizer.fit(train)

# vectorizer=TfidfVectorizer(ngram_range=(1,4))
# vectorizer.fit(train)

#文本向量化
X_train = vectorizer.transform(train)
X_test = vectorizer.transform(test)

# print(X_train)

#用来创建等差数列
weights = np.linspace(0.1, 0.9, 30)

gsc = GridSearchCV(
    estimator=LogisticRegression(max_iter=10000),
    param_grid={
        'class_weight': [{0: x, 1: 1.0-x} for x in weights]
        },
    scoring='f1',
    cv=3
)
grid_result = gsc.fit(X_train, y_train)

print("Best parameters : %s" % grid_result.best_params_)

# Plot the weights vs f1 score
dataz = pd.DataFrame({ 'score': grid_result.cv_results_['mean_test_score'],
                       'weight': weights })
dataz.plot(x='weight')
plt.show()

lr = LogisticRegression(**grid_result.best_params_, max_iter=10000)

# Fit..
lr.fit(X_train, y_train)

# Predict..
y_pred = lr.predict(X_test)

# Evaluate the model
print(classification_report(y_test, y_pred))
x=confusion_matrix(y_test,y_pred)
# print(x)
plot_confusion_matrix(lr,X_test,y_pred)
plt.show()

# plot_confusion_matrix(confusion_matrix(y_test, y_pred))
# plt.show()


from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score, confusion_matrix
from xgboost import XGBClassifier
from sklearn.neighbors import  KNeighborsClassifier

clfs = {
    'XGB':XGBClassifier(),
    'svm1': SVC(kernel='linear'),
    'mlp1': MLPClassifier(),
    'mlp2': MLPClassifier(hidden_layer_sizes=[100, 100]),
    'ada': AdaBoostClassifier(),
    'dtc': DecisionTreeClassifier(),
    'rfc': RandomForestClassifier(),
    'gbc': GradientBoostingClassifier(),
    'lr': LogisticRegression(**grid_result.best_params_, max_iter=10000),
    'knn':KNeighborsClassifier(),
    # 'GSC':GridSearchCV()
}

f1_scores = dict()
for clf_name in clfs:
    print(clf_name)
    clf = clfs[clf_name]
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    f1_scores[clf_name] = f1_score(y_test, y_pred)
    # Evaluate the model
    print(classification_report(y_test, y_pred))
    plot_confusion_matrix(clf,X_test, y_pred)
    plt.show()

